
[ {
  "id" : "cikm13",
  "title" : "Automated Probabilistic Modeling for Relational Data",
  "pubTypeSlot" : "Conference",
  "year" : 2013,
  "authorIds" : [ "sameer", "thore" ],
  "venueId" : "cikm",
  "abstractText" : "Probabilistic graphical model representations of relational data provide a number of desired features, such as inference of missing values, detection of errors, visualization of data, and probabilistic answers to relational queries. However, adoption has been slow due to the high level of expertise expected both in probability and in the domain from the user. Instead of requiring a domain expert to specify the probabilistic dependencies of the data, we present an approach that uses the relational DB schema to automatically construct a Bayesian graphical model for a database. This resulting model contains customized distributions for the attributes, latent variables that cluster the records, and factors that reflect and represent the foreign key links, whilst allowing efficient inference. Experiments demonstrate the accuracy of the model and scalability of inference on synthetic and real-world data.",
  "pdfLink" : "http://research.microsoft.com/pubs/200220/cikm1335-singh-preprint.pdf",
  "extraLinksSlot" : [ [ "Project Page", "http://research.microsoft.com/apps/pubs/default.aspx?id=200220" ] ]
}, {
  "id" : "kbp13",
  "title" : "Universal Schema for Slot Filling and Cold Start: UMass IESL at TACKBP 2013",
  "pubTypeSlot" : "Conference",
  "year" : 2013,
  "authorIds" : [ "sameer", "limin", "David Belanger", "ari", "Sam Anzaroot", "wick", "Alexandre Passos", "harshal", "jinho", "martin", "andrew" ],
  "venueId" : "tackbp",
  "abstractText" : "We employ \\textit{universal schema} for the TAC KBP slot filling and cold start tasks. The technique enlarges the set of relations in an ontology, e.g., TACKBP slots, to contain all surface patterns between pairs of entities in a large corpus. By factorizing the matrix of co-occurrences between entity pairs and universal schema relations, we are able to predict new target slots. This differs fundamentally from traditional relation extraction approaches because an entire knowledge base is constructed jointly over train and test data. To produce submissions for the slot filling and cold start tasks, we simply query this knowledge base. We describe universal schema, our data preprocessing pipeline, and additional techniques we employ for predicting entities' attributes.",
  "pdfLink" : "http://cs.umass.edu/~sameer/files/umass-tackbp13.pdf"
}, {
  "id" : "conll13",
  "title" : "Dynamic Knowledge-Base Alignment for Coreference Resolution",
  "pubTypeSlot" : "Conference",
  "year" : 2013,
  "authorIds" : [ "jiaping", "luke", "sameer", "jinho", "andrew" ],
  "venueId" : "conll",
  "abstractText" : "Coreference resolution systems can benefit greatly from inclusion of global context, and a number of recent approaches have demonstrated improvements when precomputing an alignment to external knowledge sources. However, since alignment itself is a challenging task and is often noisy, existing systems either align conservatively, resulting in very few links, or combine the attributes of multiple candidates, leading to a conflation of entities. Our approach instead performs joint inference between within-document coreference and entity linking, maintaining ranked lists of candidate entities that are dynamically merged and reranked during inference. Further, we incorporate a large set of surface string variations for each entity by using anchor texts from the web that link to the entity. These forms of global context enables our system to improve classifier-based coreference by 1.09 B3 F1 points, and improve over the previous state-of-art by 0.41 points, thus introducing a new state-of-art result on the ACE 2004 data.",
  "pdfLink" : "http://cs.umass.edu/~sameer/files/dynamic-conll13.pdf"
}, {
  "id" : "jnt:akbc13",
  "title" : "Joint Inference of Entities, Relations, and Coreference",
  "pubTypeSlot" : "Workshop",
  "year" : 2013,
  "authorIds" : [ "sameer", "sebastian", "martin", "jiaping", "andrew" ],
  "venueId" : "akbc13",
  "pdfLink" : "http://cs.umass.edu/~sameer/files/joint-akbc13.pdf"
}, {
  "id" : "conf:akbc13",
  "title" : "Assessing Confidence of Knowledge Base Content with an Experimental Study in Entity Resolution",
  "pubTypeSlot" : "Workshop",
  "year" : 2013,
  "authorIds" : [ "wick", "sameer", "ari", "andrew" ],
  "venueId" : "akbc13",
  "pdfLink" : "http://cs.umass.edu/~sameer/files/confidence-akbc13.pdf"
}, {
  "id" : "link:akbc13",
  "title" : "A Joint Model for Discovering and Linking Entities",
  "pubTypeSlot" : "Workshop",
  "year" : 2013,
  "authorIds" : [ "wick", "sameer", "harshal", "andrew" ],
  "venueId" : "akbc13",
  "pdfLink" : "http://cs.umass.edu/~sameer/files/linking-akbc13.pdf"
}, {
  "id" : "sparse:reseff13",
  "title" : "Anytime Belief Propagation Using Sparse Domains",
  "pubTypeSlot" : "Workshop",
  "year" : 2013,
  "authorIds" : [ "sameer", "sebastian", "andrew" ],
  "venueId" : "reseff",
  "abstractText" : "Belief Propagation has been widely used for marginal inference, however it is slow on problems with large-domain variables and high-order factors. Previous work provides useful approximations to facilitate inference on such models, but lacks important anytime properties such as: 1) providing accurate and consistent marginals when stopped early, 2) improving the approximation when run longer, and 3) converging to the fixed point of BP. To this end, we propose a message passing algorithm that works on sparse (partially instantiated) domains, and converges to consistent marginals using dynamic message scheduling. The algorithm grows the sparse domains incrementally, selecting the next value to add using prioritization schemes based on the gradients of the marginal inference objective. Our experiments demonstrate local anytime consistency and fast convergence, providing significant speedups over BP to obtain low-error marginals: up to 25 times on grid models, and up to 6 times on a real-world natural language processing task.",
  "pdfLink" : "http://cs.umass.edu/~sameer/files/singh-nipsws13-sparsebp.pdf",
  "extraLinksSlot" : [ [ "arXiv Page", "http://arxiv.org/abs/1311.3368" ] ]
}, {
  "id" : "mcmcmc:emnlp13",
  "title" : "Monte Carlo MCMC: Efficient Inference by Approximate Sampling",
  "pubTypeSlot" : "Conference",
  "year" : 2012,
  "authorIds" : [ "sameer", "wick", "andrew" ],
  "venueId" : "emnlp"
}, {
  "id" : "hcoref:acl12",
  "title" : "A Discriminative Hierarchical Model for Fast Coreference at Large Scale",
  "pubTypeSlot" : "Conference",
  "year" : 2012,
  "authorIds" : [ "wick", "sameer", "andrew" ],
  "venueId" : "acl"
}, {
  "id" : "mldb:probprog12",
  "title" : "Compiling Relational Database Schemata into Probabilistic Graphical Models",
  "pubTypeSlot" : "Workshop",
  "year" : 2012,
  "authorIds" : [ "sameer", "thore" ],
  "venueId" : "probprog"
}, {
  "id" : "mcmcmc:akbc12",
  "title" : "Monte Carlo MCMC: Efficient Inference by Sampling Factors",
  "pubTypeSlot" : "Workshop",
  "year" : 2012,
  "authorIds" : [ "sameer", "wick", "andrew" ],
  "venueId" : "akbc12"
}, {
  "id" : "mcmcge:tr2012",
  "title" : "Constraint-Driven Training of Complex Models Using MCMC",
  "pubTypeSlot" : "TechReport",
  "year" : 2012,
  "authorIds" : [ "sameer", "greg", "andrew" ],
  "venueId" : "University of Massachusetts Amherst, CMPSCI Technical Report, UM-CS-2012-032"
}, {
  "id" : "wlinks:tr2012",
  "title" : "Wikilinks: A Large-scale Cross-Document Coreference Corpus Labeled via Links to Wikipedia",
  "pubTypeSlot" : "TechReport",
  "year" : 2012,
  "authorIds" : [ "sameer", "amar", "fernando", "andrew" ],
  "venueId" : "University of Massachusetts Amherst, CMPSCI Technical Report, UM-CS-2012-015"
}, {
  "id" : "parfs:suml11",
  "title" : "Parallel Large-scale Feature Selection",
  "pubTypeSlot" : "Chapter",
  "year" : 2011,
  "authorIds" : [ "Jeremy Kubica", "sameer", "Daria Sorokina" ],
  "venueId" : "Scaling Up Machine Learning, Cambridge University Press"
}, {
  "id" : "dcoref:acl11",
  "title" : "Large-Scale Cross-Document Coreference Using Distributed Inference and Hierarchical Models",
  "pubTypeSlot" : "Conference",
  "year" : 2011,
  "authorIds" : [ "sameer", "amar", "fernando", "andrew" ],
  "venueId" : "acl"
}, {
  "id" : "asyncmcmc:biglearn11",
  "title" : "Towards Asynchronous Distributed MCMC Inference for Large Graphical Models",
  "pubTypeSlot" : "Workshop",
  "year" : 2011,
  "authorIds" : [ "sameer", "andrew" ],
  "venueId" : "biglearn"
}, {
  "id" : "sparsebp:cost11",
  "title" : "Inducing Value Sparsity for Parallel Inference in Tree-shaped Models",
  "pubTypeSlot" : "Workshop",
  "year" : 2011,
  "authorIds" : [ "sameer", "martin", "andrew" ],
  "venueId" : "cost"
}, {
  "id" : "cons:naacl10",
  "title" : "Constraint-Driven Rank-Based Learning for Information Extraction",
  "pubTypeSlot" : "Conference",
  "year" : 2010,
  "authorIds" : [ "sameer", "limin", "sebastian", "andrew" ],
  "venueId" : "naacl",
  "abstractText" : "Most learning algorithms for factor graphs require complete inference over the dataset or an instance before making an update to the parameters. SampleRank is a rank-based learning framework that alleviates this problem by updating the parameters during inference. Most semi-supervised learning algorithms also rely on the complete inference, i.e. calculating expectations or MAP configurations. We extend the SampleRank framework to the semi-supervised learning, avoiding these inference bottlenecks. Different approaches for incorporating unlabeled data and prior knowledge into this framework are explored. We evaluated our method on a standard information extraction dataset. Our approach outperforms the supervised method significantly and matches the result of the competing semi-supervised learning approach.Most learning algorithms for factor graphs require complete inference over the dataset or an instance before making an update to the parameters. SampleRank is a rank-based learning framework that alleviates this problem by updating the parameters during inference. Most semi-supervised learning algorithms also rely on the complete inference, i.e. calculating expectations or MAP configurations. We extend the SampleRank framework to the semi-supervised learning, avoiding these inference bottlenecks. Different approaches for incorporating unlabeled data and prior knowledge into this framework are explored. We evaluated our method on a standard information extraction dataset. Our approach outperforms the supervised method significantly and matches the result of the competing semi-supervised learning approach.",
  "pdfLink" : "http://cs.umass.edu/~sameer/files/constraint-naacl10.pdf",
  "pptLink" : "http://cs.umass.edu/~sameer/files/constraint-naacl10-ppt.pdf"
}, {
  "id" : "min:naacl10",
  "title" : "Minimally-Supervised Extraction of Entities from Text Advertisements",
  "pubTypeSlot" : "Conference",
  "year" : 2010,
  "authorIds" : [ "sameer", "Dustin Hillard", "Chris Leggetter" ],
  "venueId" : "naacl",
  "abstractText" : "Extraction of entities from ad creatives is an important problem that can benefit many computational advertising tasks. Supervised and semi-supervised solutions rely on labeled data which is expensive, time consuming, and difficult to procure for ad creatives. A small set of manually derived constraints on feature expectations over unlabeled data can be used to *partially* and *probabilistically* label large amounts of data. Utilizing recent work in constraint-based semi-supervised learning, this paper injects light weight supervision specified as these ``constraints'' into a semi-Markov conditional random field model of entity extraction in ad creatives. Relying solely on the constraints, the model is trained on a set of unlabeled ads using an online learning algorithm. We demonstrate significant accuracy improvements on a manually labeled test set as compared to a baseline dictionary approach. We also achieve accuracy that approaches a fully supervised classifier.",
  "pdfLink" : "http://cs.umass.edu/~sameer/files/minimally-naacl10.pdf",
  "pptLink" : "http://cs.umass.edu/~sameer/files/minimally-naacl10-ppt.pdf"
}, {
  "id" : "distmap:lccc10",
  "title" : "Distributed MAP Inference for Undirected Graphical Models",
  "pubTypeSlot" : "Workshop",
  "year" : 2010,
  "authorIds" : [ "sameer", "amar", "fernando", "andrew" ],
  "venueId" : "lccc",
  "abstractText" : "In this work, we distribute the MCMC-based MAP inference using the Map-Reduce framework. The variables are assigned randomly to machines, which leads to some factors that neighbor variables on separate machines. Parallel MCMC-chains are initiated using proposal distributions that only suggest local changes such that factors that lie across machines are not examined. After a fixed number of samples on each machine, we redistribute the variables amongst the machines to enable proposals across variables that were on different machines. To demonstrate the distribution strategy on a real-world information extraction application, we model the task of cross-document coreference.",
  "pdfLink" : "http://cs.umass.edu/~sameer/files/singh-nipsws10-lccc.pdf",
  "pptLink" : "http://cs.umass.edu/~sameer/files/singh-nipsws10-lccc-ppt.pdf",
  "extraLinksSlot" : [ [ "Video", "http://videolectures.net/nipsworkshops2010_singh_dmapi" ] ]
}, {
  "id" : "distantly:tr10",
  "title" : "Distantly Labeling Data for Large Scale Cross-Document Coreference",
  "pubTypeSlot" : "TechReport",
  "year" : 2010,
  "authorIds" : [ "sameer", "wick", "andrew" ],
  "venueId" : "Computing Research Repository (CoRR) eprint arXiv:1005.4298",
  "abstractText" : "Cross-document coreference, the problem of resolving entity mentions across multi-document collections, is crucial to automated knowledge base construction and data mining tasks. However, the scarcity of large labeled data sets has hindered supervised machine learning research for this task. In this paper we develop and demonstrate an approach based on ``distantly-labeling'' a data set from which we can train a discriminative cross-document coreference model. In particular we build a dataset of more than a million people mentions extracted from 3.5 years of New York Times articles, leverage Wikipedia for distant labeling with a generative model (and measure the reliability of such labeling); then we train and evaluate a conditional random field coreference model that has factors on cross-document entities as well as mention-pairs. This coreference model obtains high accuracy in resolving mentions and entities that are not present in the training data, indicating applicability to non-Wikipedia data. Given the large amount of data, our work is also an exercise demonstrating the scalability of our approach.",
  "pdfLink" : "http://arxiv.org/pdf/1005.4298v1",
  "extraLinksSlot" : [ [ "arXiv Page", "http://arxiv.org/abs/1005.4298" ] ]
} ]